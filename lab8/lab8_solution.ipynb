{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "* Owlready2 * Warning: optimized Cython parser module 'owlready2_optimized' is not available, defaulting to slower Python implementation\n"
     ]
    }
   ],
   "source": [
    "from owlready2 import *\n",
    "import pandas as pd\n",
    "from rdflib import Graph, URIRef, BNode, Literal, Namespace\n",
    "import AccessEntityLabels\n",
    "import Levenshtein as lev\n",
    "from stringcmp import isub\n",
    "from rdflib.namespace import OWL\n",
    "from CompareWithReference import compareWithReference\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getClasses(onto):        \n",
    "    return onto.classes()\n",
    "    \n",
    "def getDataProperties(onto):        \n",
    "    return onto.data_properties()\n",
    "    \n",
    "def getObjectProperties(onto):        \n",
    "    return onto.object_properties()\n",
    "    \n",
    "def getIndividuals(onto):    \n",
    "    return onto.individuals()\n",
    "\n",
    "\n",
    "def getRDFSLabelsForEntity(entity):\n",
    "    #if hasattr(entity, \"label\"):\n",
    "    return entity.label\n",
    " \n",
    "\n",
    "def extractEntities(urionto, entity_type = 'class'):\n",
    "    \n",
    "    \"\"\"\n",
    "    A function used to extract the information from a given ontology. The returned objects is an array of dictionaries, each having the 'iri', 'name' and 'labels' keys\n",
    "\n",
    "    ...\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    urionto : str\n",
    "        the owl file containing the ontology (e.g. 'cmt.owl')\n",
    "    entity_type : str\n",
    "        the type of entity to extract from the ontology. The valid values are {'class', 'objectProperty', 'dataProperty', 'individual'}. if not specified the default is 'class'\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    #Method from owlready\n",
    "    onto = get_ontology(urionto).load()\n",
    "    \n",
    "    entities = list([])\n",
    "    \n",
    "    #load the classes\n",
    "    if entity_type == 'class':\n",
    "        print(f\"Classes in {urionto} Ontology: {str(len(list(getClasses(onto))))}\")\n",
    "        entities = list(getClasses(onto))\n",
    "    \n",
    "    #...or the object properties\n",
    "    elif entity_type == 'objectProperty':\n",
    "        print(f\"Object Properties in {urionto} Ontology: {str(len(list(getObjectProperties(onto))))}\")\n",
    "        entities = getObjectProperties(onto)\n",
    "        \n",
    "    #...or the data properties\n",
    "    elif entity_type == 'dataProperty':\n",
    "        print(f\"Data Properties in {urionto} Ontology: {str(len(list(getDataProperties(onto))))}\")\n",
    "        entities = getDataProperties(onto)\n",
    "    \n",
    "    #...or the individuals from that ontology\n",
    "    elif entity_type == 'individual':\n",
    "        print(f\"Individuals in {urionto} Ontology: {str(len(list(getIndividuals(onto))))}\")\n",
    "        entities = getIndividuals(onto)\n",
    "    \n",
    "    #else if the user input is not one of the valid entity types print an error message\n",
    "    else:\n",
    "        print(\"Incorrect entity type\")\n",
    "\n",
    "    #create a new array to hold all the extracted iris, their name and their label(s). Be it for classes, properties or individuals \n",
    "    entity_dict = []\n",
    "    for entity in entities:\n",
    "        temp = {}\n",
    "        temp[\"iri\"] = entity.iri\n",
    "        temp[\"name\"] = entity.name\n",
    "        temp[\"labels\"] = getRDFSLabelsForEntity(entity)\n",
    "        entity_dict.append(temp)\n",
    "        \n",
    "        \n",
    "    return entity_dict\n",
    "\n",
    "def compare2Arrays(array_1, array_2, entity_type, entity_scores, annotation = 'name'):\n",
    "    \n",
    "    \"\"\"\n",
    "    A function used to compare 2 lists (of ontology entities) and return an third list with entity pairs and they score and their type based on the lexical comparison using the isub metric\n",
    "\n",
    "    ...\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    array_1 : list\n",
    "        the first list containing entities from the first ontology to compare\n",
    "    array_2 : list\n",
    "        the second list containing entities from the second ontology to compare\n",
    "    entity_type : str\n",
    "        the type of entity to extract from the ontology. The valid values are {'class', 'objectProperty', 'dataProperty', 'individual'}. if not specified the default is 'class'\n",
    "    entity_scores : list\n",
    "        a list containing a pair of IRIs, their entity type and the score based on the selected distance. This list will become the output as well after enriched with the new pairs\n",
    "    annotation: string\n",
    "        the \"attribute\" to be used for the lexical comparison. The valid values are {'name', 'labels'}. If not specified the default value is 'name'\n",
    "\n",
    "    \"\"\"\n",
    "    iterator = 0\n",
    "    start = time.time()\n",
    "    for i in array_1:\n",
    "        iterator += 1\n",
    "        score = 0\n",
    "        best_pair = {}\n",
    "\n",
    "        for j in array_2:\n",
    "            \n",
    "            #this part checks if we are comparing the names or the labels. that's because names are strings but labels are arrays so we need to get one level deeper\n",
    "            if annotation == 'name':\n",
    "                string1 = i[annotation]\n",
    "                string2 = j[annotation]\n",
    "            if annotation == 'labels':\n",
    "                if (len(i[annotation])>0) & (len(j[annotation])>0):\n",
    "                    string1 = i[annotation][0]\n",
    "                    string2 = j[annotation][0]\n",
    "                else:\n",
    "                    string1 = ''\n",
    "                    string2 = ''\n",
    "            \n",
    "            #only to this if both strings are not empty\n",
    "            if (len(string1)>0) & (len(string2)>0):\n",
    "                new_score = isub(string1,string2)\n",
    "                if new_score > score:\n",
    "                    score = new_score\n",
    "                    best_pair = {\"entity1\": i['iri'], \"entity2\": j['iri'], \"entity_type\": entity_type, \"score\": score}\n",
    "                    \n",
    "        entity_scores.append(best_pair)\n",
    "\n",
    "    end = time.time()\n",
    "    print(end - start)\n",
    "\n",
    "\n",
    "\n",
    "    #return the scores list\n",
    "    return entity_scores\n",
    "\n",
    "def ontologyMatcher(uri1, uri2, annotation = 'name'):\n",
    "    \n",
    "    \"\"\"\n",
    "    A function used to orchestrate the matching of two ontologies uri1 and uri2 by comparing the lexical similarity of all entities (i.e. classes and properties) based on the annotation (i.e. name or labels)\n",
    "\n",
    "    ...\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    uri1 : string\n",
    "        the name of the owl file of the first ontology to compare\n",
    "    uri2 : string\n",
    "        the name of the owl file of the second ontology to compare\n",
    "    annotation: string\n",
    "        the \"attribute\" to be used for the lexical comparison. The valid values are {'name', 'labels'}. If not specified the default value is 'name'\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    # load the classes and objects from the 2 uris in the respective arrays of objects\n",
    "    dict_uri1_classes = extractEntities(uri1,\"class\")\n",
    "    dict_uri2_classes = extractEntities(uri2,\"class\")\n",
    "    dict_uri1_obj_properties = extractEntities(uri1,\"objectProperty\")\n",
    "    dict_uri2_obj_properties = extractEntities(uri2,\"objectProperty\")\n",
    "    dict_uri1_data_properties = extractEntities(uri1,\"dataProperty\")\n",
    "    dict_uri2_data_properties = extractEntities(uri2,\"dataProperty\")\n",
    "    \n",
    "    # Create an empty array to hold the objects. each object is a dictionary with two uris and the score of the similarity of their names\n",
    "    entity_scores = []\n",
    "    \n",
    "    # compare class names and add the scores to the dictionary\n",
    "    entity_scores = compare2Arrays(dict_uri1_classes, dict_uri2_classes, 'class', entity_scores, annotation)\n",
    "    \n",
    "    # ...do the same with object properties\n",
    "    entity_scores = compare2Arrays(dict_uri1_obj_properties, dict_uri2_obj_properties, 'objectProperty', entity_scores, annotation)\n",
    "    \n",
    "    # ...do the same with data properties\n",
    "    entity_scores = compare2Arrays(dict_uri1_data_properties, dict_uri2_data_properties, 'dataProperty', entity_scores, annotation)\n",
    "    \n",
    "    # finally we convert the dictionary to a dataframe to be able to filter pairs with the score above a certain threshold\n",
    "    return pd.DataFrame(entity_scores)\n",
    "\n",
    "def createAlignmentTripples(enity_scores,threshold=0.0):\n",
    "    \n",
    "    \"\"\"\n",
    "    A function create a graph with the tripples as specified int the entity scores list\n",
    "\n",
    "    ...\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    entity_scores : list\n",
    "        a list containing a pair of IRIs, their entity type and the score based on the selected distance.\n",
    "    threshold : float\n",
    "        a number used to filter the pairs that have scored higher than the threshold and only consider them for the graph triples\n",
    "    \"\"\"\n",
    "        \n",
    "    #initialise a new graph\n",
    "    g = Graph()\n",
    "\n",
    "    g.bind(\"owl\", OWL)\n",
    "    \n",
    "    matched_onto2_entities = []\n",
    "\n",
    "    # iterate throw the rows where the score is above a certain thresholf and create the relevant triples. the score table is sorted based on scores so the pair with the highest score appears first\n",
    "    for index, row in df_entity_scores[df_entity_scores['score']>threshold].sort_values(by='score',ascending = False).iterrows():\n",
    "        \n",
    "        # we check if the entity from the second ontology has already been matched with a higher score and if it has we do not add the new pair in the graph \n",
    "        if row['entity2'] not in matched_onto2_entities:\n",
    "            \n",
    "            #we append the new entity from onto2 to the array so as do ignore it if it shows up again in lower scores\n",
    "            matched_onto2_entities.append(row['entity2'])\n",
    "            if row['entity_type'] == 'class':\n",
    "                g.add((URIRef(row['entity1']), OWL.equivalentClass, URIRef(row['entity2'])))\n",
    "            elif row['entity_type'] == 'objectProperty':\n",
    "                g.add((URIRef(row['entity1']), OWL.equivalentProperty, URIRef(row['entity2'])))\n",
    "            elif row['entity_type'] == 'dataProperty':\n",
    "                g.add((URIRef(row['entity1']), OWL.equivalentProperty, URIRef(row['entity2'])))\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes in confOf.owl Ontology: 38\n",
      "Classes in ekaw.owl Ontology: 73\n",
      "Object Properties in confOf.owl Ontology: 13\n",
      "Object Properties in ekaw.owl Ontology: 33\n",
      "Data Properties in confOf.owl Ontology: 23\n",
      "Data Properties in ekaw.owl Ontology: 0\n",
      "0.2723658084869385\n",
      "0.02991795539855957\n",
      "0.0\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Use the matcher function to compare the two ontologies and load the results in a dataframe\n",
    "# df_entity_scores = ontologyMatcher('cmt.owl', 'ekaw.owl')\n",
    "# filename = 'zdetor-cmt-ekaw.ttl'\n",
    "\n",
    "# df_entity_scores = ontologyMatcher('cmt.owl', 'confOf.owl')\n",
    "# filename = 'zdetor-cmt-confOf.ttl'\n",
    "\n",
    "df_entity_scores = ontologyMatcher('confOf.owl', 'ekaw.owl', 'name')\n",
    "filename = 'zdetor-confOf-ekaw.ttl'\n",
    "\n",
    "threshold = 0.7\n",
    "\n",
    "\n",
    "# display(df_entity_scores[df_entity_scores['score']>threshold].sort_values(by='score',ascending = False))\n",
    "\n",
    "\n",
    "# parse the dataframe with the scores and creates the triples for those pairs of entities that scored above the threshold. Add the tripples to the KG\n",
    "g = createAlignmentTripples(df_entity_scores,threshold)\n",
    "print(\"\\n\")\n",
    "#print the resulting triples in a ttl file\n",
    "# print(g.serialize(format=\"turtle\").decode(\"utf-8\"))  \n",
    "g.serialize(destination=filename, format='ttl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparing 'zdetor-cmt-ekaw.ttl' with 'cmt-ekaw-reference.ttl\n",
      "\tPrecision: 0.2727272727272727\n",
      "\tRecall: 0.5454545454545454\n",
      "\tF-Score: 0.3636363636363636\n",
      "Comparing 'zdetor-cmt-confof.ttl' with 'cmt-confOf-reference.ttl\n",
      "\tPrecision: 0.375\n",
      "\tRecall: 0.375\n",
      "\tF-Score: 0.375\n",
      "Comparing 'zdetor-confOf-ekaw.ttl' with 'confOf-ekaw-reference.ttl\n",
      "\tPrecision: 0.6296296296296297\n",
      "\tRecall: 0.85\n",
      "\tF-Score: 0.723404255319149\n"
     ]
    }
   ],
   "source": [
    "compareWithReference('cmt-ekaw-reference.ttl', \"zdetor-cmt-ekaw.ttl\")\n",
    "compareWithReference('cmt-confOf-reference.ttl', \"zdetor-cmt-confof.ttl\")\n",
    "compareWithReference('confOf-ekaw-reference.ttl', \"zdetor-confOf-ekaw.ttl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Comparing 'zdetor-mouse-human.ttl' with 'anatomy-reference.ttl\n",
      "\tPrecision: 0.838022813688213\n",
      "\tRecall: 0.7269129287598944\n",
      "\tF-Score: 0.7785234899328859\n"
     ]
    }
   ],
   "source": [
    "# df_entity_scores = ontologyMatcher('mouse.owl', 'human.owl', 'labels')\n",
    "filename = 'zdetor-mouse-human.ttl'\n",
    "\n",
    "threshold = 0.7\n",
    "\n",
    "\n",
    "\n",
    "# display(df_entity_scores[df_entity_scores['score']>threshold].sort_values(by='score',ascending = False))\n",
    "\n",
    "\n",
    "# parse the dataframe with the scores and creates the triples for those pairs of entities that scored above the threshold. Add the tripples to the KG\n",
    "g = createAlignmentTripples(df_entity_scores,threshold)\n",
    "print(\"\\n\")\n",
    "#print the resulting triples in a ttl file\n",
    "# print(g.serialize(format=\"turtle\").decode(\"utf-8\"))  \n",
    "g.serialize(destination=filename, format='ttl')\n",
    "\n",
    "compareWithReference('anatomy-reference.ttl', 'zdetor-mouse-human.ttl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The graph contains '949' triples.\n"
     ]
    }
   ],
   "source": [
    "g = Graph()\n",
    "g.parse(\"zdetor-mouse-human.ttl\", format=\"ttl\")\n",
    "    \n",
    "    \n",
    "print(\"The graph contains '\" + str(len(g)) + \"' triples.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7058823529411765\n",
      "0.0\n",
      "0.8928571428571429\n"
     ]
    }
   ],
   "source": [
    "print(lev.distance('Congo', 'Republic of Congo')/len('Republic of Congo'))\n",
    "print(lev.jaro_winkler('Congo', 'Republic of Congo'))\n",
    "print(lev.jaro_winkler('Congo', 'Congo Republic'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
