{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from owlready2 import *\n",
    "import pandas as pd\n",
    "from rdflib import Graph, URIRef, BNode, Literal, Namespace\n",
    "import AccessEntityLabels\n",
    "import Levenshtein as lev\n",
    "from stringcmp import isub\n",
    "from rdflib.namespace import OWL\n",
    "from CompareWithReference import compareWithReference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getClasses(onto):        \n",
    "    return onto.classes()\n",
    "    \n",
    "def getDataProperties(onto):        \n",
    "    return onto.data_properties()\n",
    "    \n",
    "def getObjectProperties(onto):        \n",
    "    return onto.object_properties()\n",
    "    \n",
    "def getIndividuals(onto):    \n",
    "    return onto.individuals()\n",
    "\n",
    "\n",
    "def getRDFSLabelsForEntity(entity):\n",
    "    #if hasattr(entity, \"label\"):\n",
    "    return entity.label\n",
    " \n",
    "\n",
    "def extractEntities(urionto, entity_type = 'class'):\n",
    "    \n",
    "    \"\"\"\n",
    "    A function used to extract the information from a given ontology. The returned objects is an array of dictionaries, each having the 'iri', 'name' and 'labels' keys\n",
    "\n",
    "    ...\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    urionto : str\n",
    "        the owl file containing the ontology (e.g. 'cmt.owl')\n",
    "    entity_type : str\n",
    "        the type of entity to extract from the ontology. The valid values are {'class', 'objectProperty', 'dataProperty', 'individual'}. if not specified the default is 'class'\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    #Method from owlready\n",
    "    onto = get_ontology(urionto).load()\n",
    "    \n",
    "    entities = list([])\n",
    "    \n",
    "    #load the classes\n",
    "    if entity_type == 'class':\n",
    "        print(f\"Classes in {urionto} Ontology: {str(len(list(getClasses(onto))))}\")\n",
    "        entities = list(getClasses(onto))\n",
    "    \n",
    "    #...or the object properties\n",
    "    elif entity_type == 'objectProperty':\n",
    "        print(f\"Object Properties in {urionto} Ontology: {str(len(list(getObjectProperties(onto))))}\")\n",
    "        entities = getObjectProperties(onto)\n",
    "        \n",
    "    #...or the data properties\n",
    "    elif entity_type == 'dataProperty':\n",
    "        print(f\"Data Properties in {urionto} Ontology: {str(len(list(getDataProperties(onto))))}\")\n",
    "        entities = getDataProperties(onto)\n",
    "    \n",
    "    #...or the individuals from that ontology\n",
    "    elif entity_type == 'individual':\n",
    "        print(f\"Individuals in {urionto} Ontology: {str(len(list(getIndividuals(onto))))}\")\n",
    "        entities = getIndividuals(onto)\n",
    "    \n",
    "    #else if the user input is not one of the valid entity types print an error message\n",
    "    else:\n",
    "        print(\"Incorrect entity type\")\n",
    "\n",
    "    #create a new array to hold all the extracted iris, their name and their label(s). Be it for classes, properties or individuals \n",
    "    entity_dict = []\n",
    "    for entity in entities:\n",
    "        temp = {}\n",
    "        temp[\"iri\"] = entity.iri\n",
    "        temp[\"name\"] = entity.name\n",
    "        temp[\"labels\"] = getRDFSLabelsForEntity(entity)\n",
    "        entity_dict.append(temp)\n",
    "        \n",
    "        \n",
    "    return entity_dict\n",
    "\n",
    "def compare2Arrays(array_1, array_2, entity_type, entity_scores, annotation = 'name'):\n",
    "    \n",
    "    \"\"\"\n",
    "    A function used to compare 2 lists (of ontology entities) and return an third list with entity pairs and they score and their type based on the lexical comparison using the isub metric\n",
    "\n",
    "    ...\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    array_1 : list\n",
    "        the first list containing entities from the first ontology to compare\n",
    "    array_2 : list\n",
    "        the second list containing entities from the second ontology to compare\n",
    "    entity_type : str\n",
    "        the type of entity to extract from the ontology. The valid values are {'class', 'objectProperty', 'dataProperty', 'individual'}. if not specified the default is 'class'\n",
    "    entity_scores : list\n",
    "        a list containing a pair of IRIs, their entity type and the score based on the selected distance\n",
    "    annotation: string\n",
    "        the \"attribute\" to be used for the lexical comparison. The valid values are {'name', 'labels'}. If not specified the default value is 'name'\n",
    "\n",
    "    \"\"\"\n",
    "    iterator = 0\n",
    "    for i in array_1:\n",
    "        iterator += 1\n",
    "        score = 0\n",
    "        best_pair = {}\n",
    "        for j in array_2:\n",
    "            \n",
    "            #this part checks if we are comparing the names or the labels. that's because names are strings but labels are arrays so we need to get one level deeper\n",
    "            if annotation == 'name':\n",
    "                string1 = i[annotation]\n",
    "                string2 = j[annotation]\n",
    "            if annotation == 'labels':\n",
    "                if (len(i[annotation])>0) & (len(j[annotation])>0):\n",
    "                    string1 = i[annotation][0]\n",
    "                    string2 = j[annotation][0]\n",
    "                else:\n",
    "                    string1 = ''\n",
    "                    string2 = ''\n",
    "            \n",
    "            #only to this if both strings are not empty\n",
    "            if (len(string1)>0) & (len(string2)>0):\n",
    "                if isub(string1,string2) > score:\n",
    "                    score = isub(string1,string2)\n",
    "                    best_pair = {\"entity1\": i['iri'], \"entity2\": j['iri'], \"entity_type\": entity_type, \"score\": score}\n",
    "                    \n",
    "        entity_scores.append(best_pair)\n",
    "        if (len(array_1)%(iterator*100)) == 0:\n",
    "            print(len(array_1)%(iterator*100))\n",
    "\n",
    "\n",
    "#     for i in array_1:\n",
    "#         score = 0\n",
    "#         best_pair = {}\n",
    "#         for j in array_2:\n",
    "#             score = isub(i[\"name\"],j[\"name\"])\n",
    "#             best_pair = {\"entity1\": i['iri'], \"entity2\": j['iri'], \"entity_type\": entity_type, \"score\": score}\n",
    "#             entity_scores.append(best_pair)\n",
    "\n",
    "    #return the scores array\n",
    "    return entity_scores\n",
    "\n",
    "def ontologyMatcher(uri1, uri2, annotation):\n",
    "    \n",
    "    # load the classes and objects from the 2 uris in the respective arrays of objects\n",
    "    dict_uri1_classes = extractEntities(uri1,\"class\")\n",
    "    dict_uri2_classes = extractEntities(uri2,\"class\")\n",
    "    dict_uri1_obj_properties = extractEntities(uri1,\"objectProperty\")\n",
    "    dict_uri2_obj_properties = extractEntities(uri2,\"objectProperty\")\n",
    "    dict_uri1_data_properties = extractEntities(uri1,\"dataProperty\")\n",
    "    dict_uri2_data_properties = extractEntities(uri2,\"dataProperty\")\n",
    "    \n",
    "    # Create an empty array to hold the objects. each object is a dictionary with two uris and the score of the similarity of their names\n",
    "    entity_scores = []\n",
    "    \n",
    "    # compare class names and add the scores to the dictionary\n",
    "    entity_scores = compare2Arrays(dict_uri1_classes, dict_uri2_classes, 'class', entity_scores, annotation)\n",
    "    \n",
    "    # ...do the same with object properties\n",
    "    entity_scores = compare2Arrays(dict_uri1_obj_properties, dict_uri2_obj_properties, 'objectProperty', entity_scores, annotation)\n",
    "    \n",
    "    # ...do the same with data properties\n",
    "    entity_scores = compare2Arrays(dict_uri1_data_properties, dict_uri2_data_properties, 'dataProperty', entity_scores, annotation)\n",
    "    \n",
    "    # finally we convert the dictionary to a dataframe to be able to filter pairs with the score above a certain threshold\n",
    "    return pd.DataFrame(entity_scores)\n",
    "\n",
    "def createAlignmentTripples(enity_scores,threshold):\n",
    "    #initialise a new graph\n",
    "    g = Graph()\n",
    "\n",
    "    g.bind(\"owl\", OWL)\n",
    "    \n",
    "    matched_onto2_entities = []\n",
    "\n",
    "    # iterate throw the rows where the score is above a certain thresholf and create the relevant triples. the score table is sorted based on scores so the pair with the highest score appears first\n",
    "    for index, row in df_entity_scores[df_entity_scores['score']>threshold].sort_values(by='score',ascending = False).iterrows():\n",
    "        \n",
    "        # we check if the entity from the second ontology has already been matched with a higher score and if it has we do not add the new pair in the graph \n",
    "        if row['entity2'] not in matched_onto2_entities:\n",
    "            \n",
    "            #we append the new entity from onto2 to the array so as do ignore it if it shows up again in lower scores\n",
    "            matched_onto2_entities.append(row['entity2'])\n",
    "            if row['entity_type'] == 'class':\n",
    "                g.add((URIRef(row['entity1']), OWL.equivalentClass, URIRef(row['entity2'])))\n",
    "            elif row['entity_type'] == 'objectProperty':\n",
    "                g.add((URIRef(row['entity1']), OWL.equivalentProperty, URIRef(row['entity2'])))\n",
    "            elif row['entity_type'] == 'dataProperty':\n",
    "                g.add((URIRef(row['entity1']), OWL.equivalentProperty, URIRef(row['entity2'])))\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Use the matcher function to compare the two ontologies and load the results in a dataframe\n",
    "# df_entity_scores = ontologyMatcher('cmt.owl', 'ekaw.owl')\n",
    "# filename = 'zdetor-cmt-ekaw.ttl'\n",
    "\n",
    "# df_entity_scores = ontologyMatcher('cmt.owl', 'confOf.owl')\n",
    "# filename = 'zdetor-cmt-confOf.ttl'\n",
    "\n",
    "df_entity_scores = ontologyMatcher('confOf.owl', 'ekaw.owl', 'name')\n",
    "filename = 'zdetor-confOf-ekaw.ttl'\n",
    "\n",
    "threshold = 0.8\n",
    "\n",
    "\n",
    "display(df_entity_scores[df_entity_scores['score']>threshold].sort_values(by='score',ascending = False))\n",
    "\n",
    "\n",
    "# parse the dataframe with the scores and creates the triples for those pairs of entities that scored above the threshold. Add the tripples to the KG\n",
    "g = createAlignmentTripples(df_entity_scores,threshold)\n",
    "print(\"\\n\")\n",
    "#print the resulting triples in a ttl file\n",
    "print(g.serialize(format=\"turtle\").decode(\"utf-8\"))  \n",
    "g.serialize(destination=filename, format='ttl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparing 'zdetor-cmt-ekaw.ttl' with 'cmt-ekaw-reference.ttl\n",
      "\tPrecision: 0.5454545454545454\n",
      "\tRecall: 0.5454545454545454\n",
      "\tF-Score: 0.5454545454545454\n",
      "Comparing 'zdetor-cmt-confof.ttl' with 'cmt-confOf-reference.ttl\n",
      "\tPrecision: 0.5\n",
      "\tRecall: 0.3125\n",
      "\tF-Score: 0.38461538461538464\n",
      "Comparing 'zdetor-confOf-ekaw.ttl' with 'confOf-ekaw-reference.ttl\n",
      "\tPrecision: 0.6363636363636364\n",
      "\tRecall: 0.7\n",
      "\tF-Score: 0.6666666666666666\n"
     ]
    }
   ],
   "source": [
    "compareWithReference('cmt-ekaw-reference.ttl', \"zdetor-cmt-ekaw.ttl\")\n",
    "compareWithReference('cmt-confOf-reference.ttl', \"zdetor-cmt-confof.ttl\")\n",
    "compareWithReference('confOf-ekaw-reference.ttl', \"zdetor-confOf-ekaw.ttl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Comparing 'zdetor-mouse-human.ttl' with 'anatomy-reference.ttl\n",
      "\tPrecision: 0.9525816649104321\n",
      "\tRecall: 0.5963060686015831\n",
      "\tF-Score: 0.7334685598377282\n"
     ]
    }
   ],
   "source": [
    "# df_entity_scores = ontologyMatcher('mouse.owl', 'human.owl', 'labels')\n",
    "filename = 'zdetor-mouse-human.ttl'\n",
    "\n",
    "threshold = 0.8\n",
    "\n",
    "\n",
    "# display(df_entity_scores[df_entity_scores['score']>threshold].sort_values(by='score',ascending = False))\n",
    "\n",
    "\n",
    "# parse the dataframe with the scores and creates the triples for those pairs of entities that scored above the threshold. Add the tripples to the KG\n",
    "g = createAlignmentTripples(df_entity_scores,threshold)\n",
    "print(\"\\n\")\n",
    "#print the resulting triples in a ttl file\n",
    "# print(g.serialize(format=\"turtle\").decode(\"utf-8\"))  \n",
    "g.serialize(destination=filename, format='ttl')\n",
    "\n",
    "compareWithReference('anatomy-reference.ttl', 'zdetor-mouse-human.ttl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object Properties in cmt.owl Ontology: 49\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'iri': 'http://cmt#assignedByReviewer',\n",
       "  'name': 'assignedByReviewer',\n",
       "  'labels': []},\n",
       " {'iri': 'http://cmt#hasDecision', 'name': 'hasDecision', 'labels': []},\n",
       " {'iri': 'http://cmt#readByReviewer', 'name': 'readByReviewer', 'labels': []},\n",
       " {'iri': 'http://cmt#readByMeta-Reviewer',\n",
       "  'name': 'readByMeta-Reviewer',\n",
       "  'labels': []},\n",
       " {'iri': 'http://cmt#hasConflictOfInterest',\n",
       "  'name': 'hasConflictOfInterest',\n",
       "  'labels': []},\n",
       " {'iri': 'http://cmt#finalizePaperAssignment',\n",
       "  'name': 'finalizePaperAssignment',\n",
       "  'labels': []},\n",
       " {'iri': 'http://cmt#paperAssignmentFinalizedBy',\n",
       "  'name': 'paperAssignmentFinalizedBy',\n",
       "  'labels': []},\n",
       " {'iri': 'http://cmt#hasConferenceMember',\n",
       "  'name': 'hasConferenceMember',\n",
       "  'labels': []},\n",
       " {'iri': 'http://cmt#memberOfConference',\n",
       "  'name': 'memberOfConference',\n",
       "  'labels': []},\n",
       " {'iri': 'http://cmt#runPaperAssignmentTools',\n",
       "  'name': 'runPaperAssignmentTools',\n",
       "  'labels': []},\n",
       " {'iri': 'http://cmt#paperAssignmentToolsRunBy',\n",
       "  'name': 'paperAssignmentToolsRunBy',\n",
       "  'labels': []},\n",
       " {'iri': 'http://cmt#enableVirtualMeeting',\n",
       "  'name': 'enableVirtualMeeting',\n",
       "  'labels': []},\n",
       " {'iri': 'http://cmt#virtualMeetingEnabledBy',\n",
       "  'name': 'virtualMeetingEnabledBy',\n",
       "  'labels': []},\n",
       " {'iri': 'http://cmt#hasProgramCommitteeMember',\n",
       "  'name': 'hasProgramCommitteeMember',\n",
       "  'labels': []},\n",
       " {'iri': 'http://cmt#memberOfProgramCommittee',\n",
       "  'name': 'memberOfProgramCommittee',\n",
       "  'labels': []},\n",
       " {'iri': 'http://cmt#startReviewerBidding',\n",
       "  'name': 'startReviewerBidding',\n",
       "  'labels': []},\n",
       " {'iri': 'http://cmt#reviewerBiddingStartedBy',\n",
       "  'name': 'reviewerBiddingStartedBy',\n",
       "  'labels': []},\n",
       " {'iri': 'http://cmt#assignReviewer', 'name': 'assignReviewer', 'labels': []},\n",
       " {'iri': 'http://cmt#assignedByAdministrator',\n",
       "  'name': 'assignedByAdministrator',\n",
       "  'labels': []},\n",
       " {'iri': 'http://cmt#reviewCriteriaEnteredBy',\n",
       "  'name': 'reviewCriteriaEnteredBy',\n",
       "  'labels': []},\n",
       " {'iri': 'http://cmt#enterReviewCriteria',\n",
       "  'name': 'enterReviewCriteria',\n",
       "  'labels': []},\n",
       " {'iri': 'http://cmt#hasCo-author', 'name': 'hasCo-author', 'labels': []},\n",
       " {'iri': 'http://cmt#co-writePaper', 'name': 'co-writePaper', 'labels': []},\n",
       " {'iri': 'http://cmt#rejectPaper', 'name': 'rejectPaper', 'labels': []},\n",
       " {'iri': 'http://cmt#rejectedBy', 'name': 'rejectedBy', 'labels': []},\n",
       " {'iri': 'http://cmt#endReview', 'name': 'endReview', 'labels': []},\n",
       " {'iri': 'http://cmt#hasBid', 'name': 'hasBid', 'labels': []},\n",
       " {'iri': 'http://cmt#adjustBid', 'name': 'adjustBid', 'labels': []},\n",
       " {'iri': 'http://cmt#adjustedBy', 'name': 'adjustedBy', 'labels': []},\n",
       " {'iri': 'http://cmt#readPaper', 'name': 'readPaper', 'labels': []},\n",
       " {'iri': 'http://cmt#hardcopyMailingManifestsPrintedBy',\n",
       "  'name': 'hardcopyMailingManifestsPrintedBy',\n",
       "  'labels': []},\n",
       " {'iri': 'http://cmt#printHardcopyMailingManifests',\n",
       "  'name': 'printHardcopyMailingManifests',\n",
       "  'labels': []},\n",
       " {'iri': 'http://cmt#markConflictOfInterest',\n",
       "  'name': 'markConflictOfInterest',\n",
       "  'labels': []},\n",
       " {'iri': 'http://cmt#enterConferenceDetails',\n",
       "  'name': 'enterConferenceDetails',\n",
       "  'labels': []},\n",
       " {'iri': 'http://cmt#detailsEnteredBy',\n",
       "  'name': 'detailsEnteredBy',\n",
       "  'labels': []},\n",
       " {'iri': 'http://cmt#hasBeenAssigned',\n",
       "  'name': 'hasBeenAssigned',\n",
       "  'labels': []},\n",
       " {'iri': 'http://cmt#assignedTo', 'name': 'assignedTo', 'labels': []},\n",
       " {'iri': 'http://cmt#assignExternalReviewer',\n",
       "  'name': 'assignExternalReviewer',\n",
       "  'labels': []},\n",
       " {'iri': 'http://cmt#setMaxPapers', 'name': 'setMaxPapers', 'labels': []},\n",
       " {'iri': 'http://cmt#hasSubjectArea', 'name': 'hasSubjectArea', 'labels': []},\n",
       " {'iri': 'http://cmt#submitPaper', 'name': 'submitPaper', 'labels': []},\n",
       " {'iri': 'http://cmt#acceptPaper', 'name': 'acceptPaper', 'labels': []},\n",
       " {'iri': 'http://cmt#acceptedBy', 'name': 'acceptedBy', 'labels': []},\n",
       " {'iri': 'http://cmt#hasAuthor', 'name': 'hasAuthor', 'labels': []},\n",
       " {'iri': 'http://cmt#writePaper', 'name': 'writePaper', 'labels': []},\n",
       " {'iri': 'http://cmt#addedBy', 'name': 'addedBy', 'labels': []},\n",
       " {'iri': 'http://cmt#addProgramCommitteeMember',\n",
       "  'name': 'addProgramCommitteeMember',\n",
       "  'labels': []},\n",
       " {'iri': 'http://cmt#writtenBy', 'name': 'writtenBy', 'labels': []},\n",
       " {'iri': 'http://cmt#writeReview', 'name': 'writeReview', 'labels': []}]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extractEntities('cmt.owl', 'objectProperty')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
